S3: Simple storage service (s3) - > We can store the files and retrieve objects in S3. 
In s3, what we store in a bucket is an  "Object," not a file or block.
----------------
Create Bucket : 
->  s3 is region-specific. We can view objects across the region, but to access objects across regions or edit them, we need to enable Cross-Region Replication (CRR).
-> In s3, " Bucket name " should be unique.
< -- > In S3, we will get 2 types of object Ownership 
   -  ACLs disabled (), ACL enabled.
< -- > Then we have the " Block Public access setting for this bucket " -> by default, it will block all public access. 
< -- > Bucket Versioning
< -- > Encryption type - Used to encrypt the data stored in s3
< -- > Bucket key 

- > S3 By default, it will store our data in 3 AZ. So that if one AZ goes down, data will be stored there in other regions.

Rules & Policies : 
--- > The S3 bucket name should be unique.
--- > Buckets are region-specific.
--- > S3 object size, a single object can be uploaded up to 5TB not more than 5TB and we can upload multiple objects.
-----------------------------------------------------------
s3 Storage class (this will be by default) 
   1. S3 Standard - Use case: Frequently accessed data.
        - High Durability, -low latency
        -  Cost: Higher cost than other classes due to its fast access.
       Example use case: Websites, applications, and big data analytics.
   2. S3 Standard-IA (Infrequent Access): Use case: Less frequently accessed data that requires rapid access when needed.
         - Lower cost than Standard, but with retrieval costs. Suitable for backups, disaster recovery, and data that is 
            accessed less often but needs to be retrieved quickly
         - Cost-effective, highly available. 
         - Cost: Lower cost than Standard but higher retrieval costs.
         - Example use case: Backups, disaster recovery data, or long-term storage that is not frequently accessed.
   3. S3 Intelligent-Tiering: Use case: Data with unpredictable access patterns.
      - Automatically moves data between two access tiers (frequent and infrequent access) based on changing access patterns.
      - Ideal for data with unknown or changing access patterns.
      - Cost: Slightly higher than S3 Standard, but automatically moves data between two access tiers (frequent and    
           infrequent) to optimize costs.
      - Example use case: Data where access patterns are unpredictable, like logs, backups, or datasets for machine learning.
   4.  S3 One Zone-IA
       - infrequently accessed data that doesn't need to be stored across multiple Availability Zones.
       - Lower cost than Standard-IA, but data is stored in a single Availability Zone, so it is less resilient.
       - Suitable for data that can be recreated or is not critical for high availability.
       - Cost: Lower than S3 Standard-IA since it stores data in a single Availability Zone.
        Example use case: Storing secondary backups or data that is easily recreatable.

  5.   S3 Glacier Instant Retrieval (mainly used for archiving data )
      - Retrieval Time: Instant access to data, meaning you can retrieve objects within milliseconds, similar to S3 Standard.
      - Cost: More affordable than S3 Standard and S3 Standard-IA (Infrequent Access) for storing archived data. However, 
         retrieval is not free, and the cost per retrieval might be higher than in other classes like Standard-IA.
   6. S3 Glacier Flexible retrieval 
      - archival data that is rarely accessed and requires retrieval times ranging from minutes to hours.
      - Cost: Very low storage cost but high retrieval cost.
       Example use case: Long-term backup, archival data, or compliance data retention.
   7. S3 Glacier Deep Archive
      - Use case: Long-term archival storage with rare access.
      - Performance: Retrieval can take up to 12 hours.
      - Cost: Extremely low storage cost, the lowest among S3 classes, but retrieval costs can be high.
      - Example use case: Data that is rarely accessed, such as legal archives or compliance data with retention policies.
  8.  S3 Outposts
      Use case: Data that needs to be stored on-premises, near your on-premises applications.
------------------------------------------------------------------------------------------------------
s3 requestor pay : 
Account 1 (You): Store data in an S3 bucket (with Requester Pays enabled).
Account 2 (Another person): Accesses data from your S3 bucket.
Without Requester Pays: Account 1 pays both storage and transfer costs, even if someone else accesses the data.
With Requester Pays enabled: Account 1 only pays for storage, and Account 2 is responsible for the transfer costs when accessing the data.
Requester Pays only applies when data is transferred out of S3 (for example, downloading data or transferring it to another AWS service).
Both the bucket owner and the requester must ensure that the requester includes the x-amz-request-payer: requester header in their requests when accessing the bucket.
    s3 -> bucket(select Bucket ) -> properties -> requester pay -> enable  -> save changes.  
------------------------------------------------------------------------------------------
s3 Object tagging: is a feature that allows you to assign metadata to your objects in the form of key-value pairs. 
Amazon S3 > Buckets >Bucket name > Object > Properties > tggging.
We can create up to 10 tags for a s3 object.
--------------------------------------------------------------------------------
S3 Bucket Policy : 
By default, we will block public access to the buckets so that no one from the internet can access the bucket using the bucket URL.
TO make it public, we need to uncheck this " Block public access (bucket settings) " and then to all access to all the objects present in that bucket, we need to add a policy to that 
- > Bucket -> Permissions -> Bucket policy - > (Generate a policy and create a bucket policy)
{
    "Version": "2012-10-17",
    "Id": "Policy1741070017227",
    "Statement": [
        {
            "Sid": "Stmt1741070015695",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::testing879878t6899p87/*"( Make sure that we have added " /* " at the end of the Resourse ARN as it need to provide permosiions to all the objets in the bucket.
}
    ]
}
------------------------------------------------------------------------------------------------
To select objects only : 
{
    "Version": "2012-10-17",
    "Id": "Policy1741070017227",
    "Statement": [
        {
            "Sid": "Stmt1741070015695",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::demo-terraform-eks-state-s3-bucket-eu-north-1/terraform.tfstate",
                "arn:aws:s3:::demo-terraform-eks-state-s3-bucket-eu-north-1/Multi-Tier-With-Database.pdf"
            ]
        }
    ]
}
-----------------------------------------------------------------------------------------------
Pre-signed URL: Once we create an s3 bucket and upload some files to the s3 bucket, we can share the objects using the pre-signed URL.
This URL will have some conditions: 1. Time bound (like this URL will be accessible only for 2 hr or 10 mins)
-----
No, your friend does not need to have their own access to your S3 bucket to use a pre-signed URL. The URL itself grants temporary access to the specific file. They can access it directly as long as the URL is valid and has not expired.
----------
Bucket -> Object -> Actions ->  presigned URL -> (select the TIME) 
The pre-signed URL includes a special token that authenticates access, meaning the recipient does not need to have direct access to your S3 bucket.
--------------------------------------------------------------------------------------------------------------------
Encription in S3 : 
- > Encryption in Amazon S3 (Simple Storage Service) helps protect your data at rest (when stored) and in transit (when 
    moving between your application and S3).
-> At Rest = Data that is stored in a system and is not currently being used or transferred.
  Encryption at Rest = Encrypting data while it's sitting idle in storage (like files in S3), ensuring make sure that unauthorized users cannot access or read it.
Note: The important thing about "rest" is that the data is not being actively used at the moment but is still valuable and needs protection while it sits in storage.
There are different types of encryptions in s3 :
1. SSE-s3 encryption: This is a feature provided by Amazon Web Services (AWS) that automatically encrypts your data when it is stored in an S3 bucket.
Decryption: When we are using SSE-S3 (Server-Side Encryption with Amazon S3-Managed Keys) to encrypt data, decryption is handled automatically by Amazon S3 when you access your data. You don’t need to manually decrypt the data yourself.
 - >Data Security: The encryption helps protect your data from unauthorized access, ensuring that only authorized users or 
    services can access the plaintext data.
2. AWS s3 SSE-KMS: For this, we need a KMS key to encrypt and decrypt the data.
   Bucket -> Properties -> Default encryption -> edit -> " Server-side encryption with AWS Key Management Service keys (SSE-KMS) " -> we need to select our KMS keys(either create or use existing).
-----------------------------------------------------------------------------------------------------------------
In transist encryption: In Amazon S3, in-transit encryption refers to the encryption of data while it is being transferred between the client and the S3 service. This ensures that data is protected from unauthorized access during transmission, preventing man-in-the-middle attacks or eavesdropping while data is moving over networks.
 -> By default, Amazon S3 uses HTTPS for encryption in transit when you interact with it. This means that all connections to Amazon S3 via the internet will be encrypted using SSL/TLS (Secure Sockets Layer/Transport Layer Security) when you use the HTTPS endpoint
How in-transit encryption is used:
HTTPS (SSL/TLS): In-transit encryption in S3 is typically achieved using SSL (Secure Sockets Layer) or TLS (Transport Layer Security) protocols. When you upload or download data from an S3 bucket, the connection between your client and S3 is encrypted using these protocols if you're using the HTTPS endpoint for the S3 service (rather than HTTP). This ensures that the data sent over the network is secure.

**-**HTTP does not provide encryption, meaning the data transferred over HTTP is sent in plain text.
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnforceSSLRequests",
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::your-bucket-name/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false" (This will make HTTP request transition) if we want to (Https then we need to add as true) 
##
In short, setting "aws:SecureTransport": "true" ensures that only HTTPS requests are allowed, and any HTTP requests will be rejected.
Note : 
"aws:SecureTransport": "false": Denies requests made over HTTP, only allowing HTTPS requests.
"aws:SecureTransport": "true": Denies requests made over HTTPS, only allowing HTTP requests
##
        }
      }
    }
  ]
}
===========================================================================================================================
Bucket Versioning: Bucket Versioning is a great feature to protect against data loss, accidental deletion, or overwriting, and it can be used for a variety of use cases, including backup, recovery, compliance, and auditing.
1. Protect Objects from Accidental Deletion or Overwriting
2. Keep Multiple Versions of an Object &  Rollback to a Previous Version
3. Cross-Region Replication with Versioning
  -> You can enable Cross-Region Replication (CRR) with versioning enabled, which allows you to replicate all object 
     versions, including the delete markers, from one S3 bucket to another in a different AWS region.
     This provides a way to ensure that all versions of your data are backed up in another location for disaster recovery 
     purposes.
BUT ::::::::::::: 
If you used the option to permanently delete objects in Amazon S3 and confirmed the deletion by typing "permanently delete" (through the S3 Console or via API/CLI), then the object is permanently deleted, and the associated versions are no longer recoverable through Amazon S3.
============================================================================================================================
Bucker Replication & Batch Job : 
Replication Rule: In the case of Amazon S3, replication is used to ensure that data is automatically copied from one bucket (source bucket) to another (destination bucket) based on defined rules.

Replicating data from source to destination : 
1. Another bucker in the same region and account.   2. Another bucket in the same account but a different region.
3. Another bucket in a different region.
How to enable it: buckets(source) -> management -> replication rules -> create replication rule -> Status(what will be the status of the rule immediately after creating ) -> filter type (all objects or specific objects)-> Destination(cross region replication or same region replication) - > IAM role(as we need to communicate with other buckets we need to provide permission) -> encryption -> Destination storage class.
-----
Note that once we create this, all the files created in the source will be stored or push to the destination automatically.
----------------------------------------------------------------------------------------
Cloud front distribution: Amazon CloudFront is a Content Delivery Network (CDN) service offered by AWS (Amazon Web Services) that helps deliver content to end users with low latency and high transfer speeds. CloudFront caches your content at multiple edge locations globally, reducing the distance between your server and the user.
-> CloudFront caches static content (like HTML, CSS, JavaScript, images) in edge locations. This reduces the load on your origin servers and speeds up the delivery of content.
-> CloudFront supports HTTPS with SSL/TLS encryption, which ensures that data transferred between your users and the edge locations is secure.
There are two types of distributions:
Web Distribution: Used for delivering web content (e.g., HTML, CSS, JavaScript, images).
RTMP Distribution: Used for streaming media files.
------------------------------------------------------------------------------------------------------
CloudFront vs Cross-Region Replication (CRR)
=====
CloudFront:

Caching at Edge Locations:
You are correct that CloudFront doesn't store data at edge locations permanently. It caches content temporarily based on TTL (Time to Live) settings.
Once the TTL expires, CloudFront will fetch the content again from the origin (e.g., an S3 bucket, web server) if needed. If the origin is down and there’s no fresh content in the cache, CloudFront can no longer serve the data.
Disaster Recovery:

In case of a disaster (e.g., if your origin is down), the cached content at the edge locations would be lost once it expires. This is because CloudFront doesn’t keep permanent copies of the content in edge locations; it only caches it temporarily.
The cache expiration is driven by the TTL settings you define for your CloudFront distribution.
Accessing Data from CloudFront:

If you want to manually retrieve the cached data, you can do so through the CloudFront URL, but this is only for content that is cached at the time.
Once the cache expires, the data is no longer available via CloudFront unless it’s fetched again from the origin, which may not be available in the case of a disaster.
----------------------------------------
Cross-Region Replication (CRR):
Permanent Data Storage:
CRR is an S3 feature where objects are automatically replicated from one S3 bucket (the source) to another S3 bucket in a different AWS region (the destination).
Once the data is replicated, it is stored permanently in the destination bucket. Even if there’s a disaster at the source bucket’s region, the data will exist in the destination region’s S3 bucket.
Disaster Recovery:
CRR is a disaster recovery solution since data in one region is mirrored in another. If the primary region is down or inaccessible, you can continue to access the replicated data in the other region.
Comparison of CloudFront vs. CRR:
CloudFront: A content delivery network (CDN) used for distributing static and dynamic content. Data is cached at edge locations temporarily and expires based on TTL settings. It is not a permanent storage solution.

CRR: A replication solution for S3, designed to provide permanent copies of data across different AWS regions, offering redundancy and disaster recovery.

Key Correction Points:
CloudFront does not permanently store data at the edge. It only caches data temporarily, and when the cache expires, the data is lost unless it's fetched from the origin.
Cross-Region Replication (CRR) ensures that data is permanently replicated and stored across regions, making it suitable for disaster recovery.
Hope that clears things up! Let me know if you need further details.   
====================-----------------------------========================================================================
Cloud Front: This will not store the data in edge location, but it will have some cache rules based on that it will temprarly cache the data. Once the cache is expired, the data will also expire. 
If there is any disaster, the data in the edge location will be there only until the cache expires; once it expires, data will also be lost.
The cache expiration is based on the TTL (Time to Live) set for the cached content. Once the cached content expires, CloudFront will no longer be able to serve that content because it can’t fetch the data from the origin (since the origin is down).
We can not recover the data directly from the edge location as it doesn't store any data in the edge location; it was just a cache.
If you need to manually retrieve the data, you could access it through the public CloudFront URL, download it, and then upload it back to the master S3 bucket manually.
Used in which services: Amazon S3, API Gateway, Amazon Elastic Load Balancer (ELB), AppSync, CloudFormation, CloudTrail, Elastic Beanstalk

Cross-Region Replication (CRR): In this, we will set rules once it was triggered the data will store in all the replicas permanently. If any disaster happens in the master, data will still exist in the replicas.

Used in: s3.












