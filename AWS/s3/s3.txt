S3: Simple storage service (s3) - > We can store the files and retrieve objects in S3. 
In s3, what we store in a bucket is an  "Object" not a file or block.
----------------
Create Bucket : 
->  s3 is region-specific, We can view objects across the region but to access objects across regions or edit them we need to enable Cross-Region Replication (CRR).
-> in s3 " Bucket name " should be unique.
< -- > In S3 we will get 2 types of object Ownership 
   -  ACLs disabled (), ACL enabled.
< -- > Then we have the " Block Public access setting for this bucket " -> by default it will block all public access. 
< -- > Bucket Versioning
< -- > Encryption type - Used to encrypt the data stored in s3
< -- > Bucket key 

- > S3 By default will store our data in 3 AZ. So that if one AZ gets down data will be stored there in other regions.

Rules & Policies : 
--- > The S3 bucket name should be unique.
--- > Buckets are regional-specific.
--- > S3 object size, a single object can be uploaded up to 5TB not more than 5TB and we can upload multiple objects.
-----------------------------------------------------------
s3 Storage class : (this will be by default) 
   1. S3 Standard - Use case: Frequently accessed data.
        - High Durability, -low latency
        -  Cost: Higher cost than other classes due to its fast access.
       Example use case: Websites, applications, and big data analytics.
   2. S3 Standard-IA (Infrequent Access): Use case: Less frequently accessed data that requires rapid access when needed.
         - Lower cost than Standard, but with retrieval costs. Suitable for backups, disaster recovery, and data that is 
            accessed less often but needs to be retrieved quickly
         - Cost-effective, highly available. 
         - Cost: Lower cost than Standard but higher retrieval costs.
         - Example use case: Backups, disaster recovery data, or long-term storage that is not frequently accessed.
   3. S3 Intelligent-Tiering: Use case: Data with unpredictable access patterns.
      - Automatically moves data between two access tiers (frequent and infrequent access) based on changing access patterns.
      - Ideal for data with unknown or changing access patterns.
      - Cost: Slightly higher than S3 Standard, but automatically moves data between two access tiers (frequent and    
           infrequent) to optimize costs.
      - Example use case: Data where access patterns are unpredictable, like logs, backups, or datasets for machine learning.
   4.  S3 One Zone-IA
       - infrequently accessed data that doesn't need to be stored across multiple Availability Zones.
       - Lower cost than Standard-IA, but data is stored in a single Availability Zone, so it is less resilient.
       - Suitable for data that can be recreated or is not critical for high availability.
       - Cost: Lower than S3 Standard-IA since it stores data in a single Availability Zone.
        Example use case: Storing secondary backups, or data that is easily recreatable.

  5.   S3 Glacier Instant Retrieval (mainly used for archiving data )
      - Retrieval Time: Instant access to data, meaning you can retrieve objects within milliseconds, similar to S3 Standard.
      - Cost: More affordable than S3 Standard and S3 Standard-IA (Infrequent Access) for storing archived data. However, 
         retrieval is not free, and the cost per retrieval might be higher than in other classes like Standard-IA.
   6. S3 Glacier Flexible retrieval 
      - archival data that is rarely accessed and requires retrieval times ranging from minutes to hours.
      - Cost: Very low storage cost but high retrieval cost.
       Example use case: Long-term backup, archival data, or compliance data retention.
   7. S3 Glacier Deep Archive
      - Use case: Long-term archival storage with rare access.
      - Performance: Retrieval can take up to 12 hours.
      - Cost: Extremely low storage cost, the lowest among S3 classes, but retrieval costs can be high.
      - Example use case: Data that is rarely accessed, such as legal archives or compliance data with retention policies.
  8.  S3 Outposts
      Use case: Data that needs to be stored on-premises, near your on-premises applications.
------------------------------------------------------------------------------------------------------
s3 requestor pay : 
Account 1 (You): Store data in an S3 bucket (with Requester Pays enabled).
Account 2 (Another person): Accesses data from your S3 bucket.
Without Requester Pays: Account 1 pays both storage and transfer costs, even if someone else accesses the data.
With Requester Pays enabled: Account 1 only pays for storage, and Account 2 is responsible for the transfer costs when accessing the data.
Requester Pays only applies when data is transferred out of S3 (for example, downloading data or transferring it to another AWS service).
Both the bucket owner and the requester must ensure that the requester includes the x-amz-request-payer: requester header in their requests when accessing the bucket.
    s3 -> bucket(select Bucket ) -> properties -> requester pay -> enable  -> save changes.  
------------------------------------------------------------------------------------------
s3 Object tagging: is a feature that allows you to assign metadata to your objects in the form of key-value pairs. 
Amazon S3 > Buckets >Bucket name > Object > Properties > tggging.
We can create up to 10 tags for a s3 object.
--------------------------------------------------------------------------------
S3 Bucket Policy : 
By default, we will block public access to the buckets so that no one from the internet can access the bucket using the bucket URL.
TO make it public, we need to uncheck this " Block public access (bucket settings) " and then to all access to all the objects present in that bucket we need to add a policy to that 
- > Bucket -> Permissions -> Bucket policy - > (Generate a policy and create a bucket policy)
{
    "Version": "2012-10-17",
    "Id": "Policy1741070017227",
    "Statement": [
        {
            "Sid": "Stmt1741070015695",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": "arn:aws:s3:::testing879878t6899p87/*"( Make sure that we have added " /* " at the end of the Resourse ARN as it need to provide permosiions to all the objets in the bucket.
}
    ]
}
------------------------------------------------------------------------------------------------
To selected objects only : 
{
    "Version": "2012-10-17",
    "Id": "Policy1741070017227",
    "Statement": [
        {
            "Sid": "Stmt1741070015695",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::demo-terraform-eks-state-s3-bucket-eu-north-1/terraform.tfstate",
                "arn:aws:s3:::demo-terraform-eks-state-s3-bucket-eu-north-1/Multi-Tier-With-Database.pdf"
            ]
        }
    ]
}
-----------------------------------------------------------------------------------------------
Pre-signed URL: Once we create an s3 bucket and upload some files to the s3 bucket, we can share the objects using the pre-signed URL.
This URL will have some conditions: 1. Time bound (like this URL will be accessible only for 2 hr or 10 mins)
-----
No, your friend does not need to have their own access to your S3 bucket to use a pre-signed URL. The URL itself grants temporary access to the specific file. They can access it directly as long as the URL is valid and has not expired.
----------
Bucket -> Object -> Actions ->  presigned URL -> (select the TIME) 
The pre-signed URL includes a special token that authenticates access, meaning the recipient does not need to have direct access to your S3 bucket.
--------------------------------------------------------------------------------------------------------------------
Encription in S3 : 
- > Encryption in Amazon S3 (Simple Storage Service) helps protect your data at rest (when stored) and in transit (when 
    moving between your application and S3).
-> At Rest = Data that is stored in a system and is not currently being used or transferred.
  Encryption at Rest = Encrypting data while it's sitting idle in storage (like files in S3), ensuring make sure that unauthorized users cannot access or read it.
Note: The important thing about "rest" is that the data is not being actively used at the moment but is still valuable and needs protection while it sits in storage.
There are different types of encryptions in s3 :
1. SSE-s3 encryption: This is a feature provided by Amazon Web Services (AWS) that automatically encrypts your data when it is stored in an S3 bucket.
Decryption: When we are using SSE-S3 (Server-Side Encryption with Amazon S3-Managed Keys) to encrypt data, decryption is handled automatically by Amazon S3 when you access your data. You donâ€™t need to manually decrypt the data yourself.
 - >Data Security: The encryption helps protect your data from unauthorized access, ensuring that only authorized users or 
    services can access the plaintext data.
2. AWS s3 SSE-KMS: For this, we need a KMS key to encrypt and decrypt the data.
   Bucket -> Properties -> Default encryption -> edit -> " Server-side encryption with AWS Key Management Service keys (SSE-KMS) " -> we need to select our KMS keys(either create or use existing).
-----------------------------------------------------------------------------------------------------------------
In transist encryption: In Amazon S3, in-transit encryption refers to the encryption of data while it is being transferred between the client and the S3 service. This ensures that data is protected from unauthorized access during transmission, preventing man-in-the-middle attacks or eavesdropping while data is moving over networks.
 -> By default, Amazon S3 uses HTTPS for encryption in transit when you interact with it. This means that all connections to Amazon S3 via the internet will be encrypted using SSL/TLS (Secure Sockets Layer/Transport Layer Security) when you use the HTTPS endpoint
How in-transit encryption is used:
HTTPS (SSL/TLS): In-transit encryption in S3 is typically achieved using SSL (Secure Sockets Layer) or TLS (Transport Layer Security) protocols. When you upload or download data from an S3 bucket, the connection between your client and S3 is encrypted using these protocols if you're using the HTTPS endpoint for the S3 service (rather than HTTP). This ensures that the data sent over the network is secure.

**-**HTTP does not provide encryption, meaning the data transferred over HTTP is sent in plain text.
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "EnforceSSLRequests",
      "Effect": "Deny",
      "Action": "s3:*",
      "Resource": "arn:aws:s3:::your-bucket-name/*",
      "Condition": {
        "Bool": {
          "aws:SecureTransport": "false" (This will make HTTP request transition) if we want to (Https then we need to add as true) 
##
In short, setting "aws:SecureTransport": "true" ensures that only HTTPS requests are allowed, and any HTTP requests will be rejected.
Note : 
"aws:SecureTransport": "false": Denies requests made over HTTP, only allowing HTTPS requests.
"aws:SecureTransport": "true": Denies requests made over HTTPS, only allowing HTTP requests
##
        }
      }
    }
  ]
}
===========================================================================================================================
Bucket Versioning: Bucket Versioning is a great feature to protect against data loss, accidental deletion, or overwriting, and it can be used for a variety of use cases, including backup, recovery, compliance, and auditing.
1. Protect Objects from Accidental Deletion or Overwriting
2. Keep Multiple Versions of an Object &  Rollback to a Previous Version
3. Cross-Region Replication with Versioning
  -> You can enable Cross-Region Replication (CRR) with versioning enabled, which allows you to replicate all object 
     versions, including the delete markers, from one S3 bucket to another in a different AWS region.
     This provides a way to ensure that all versions of your data are backed up in another location for disaster recovery 
     purposes.
BUT ::::::::::::: 
If you used the option to permanently delete objects in Amazon S3 and confirmed the deletion by typing "permanently delete" (through the S3 Console or via API/CLI), then the object is permanently deleted, and the associated versions are no longer recoverable through Amazon S3.
============================================================================================================================











